{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 2\n",
      "Shape of each embedding: (1,)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "from deepface import DeepFace\n",
    "from skimage import exposure\n",
    "\n",
    "\n",
    "# Function for super-resolution (using bicubic interpolation as an example)\n",
    "def super_resolution(image, scale_factor=2):\n",
    "    return cv2.resize(image, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "# Function for contrast enhancement using CLAHE\n",
    "def enhance_contrast(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image_path = \"./datasets/people/BD09DAF0-84DC-437F-8230-3323B7614C0F_1_105_c.jpeg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Detect faces using RetinaFace\n",
    "faces = RetinaFace.detect_faces(image_path)\n",
    "\n",
    "def get_embeddings(image_path):\n",
    "    embeddings = []\n",
    "    image = cv2.imread(image_path)\n",
    "    faces = RetinaFace.detect_faces(image)\n",
    "    for face_key in faces:\n",
    "        face = faces[face_key]\n",
    "        facial_area = face[\"facial_area\"]\n",
    "\n",
    "        # Extract face ROI\n",
    "        x, y, w, h = facial_area\n",
    "        face_image = image[y : y + h, x : x + w]\n",
    "\n",
    "        # Apply super-resolution\n",
    "        face_image = super_resolution(face_image)\n",
    "\n",
    "        # Enhance contrast\n",
    "        face_image = enhance_contrast(face_image)\n",
    "\n",
    "        # Get face embedding using DeepFace (with ArcFace model)\n",
    "        embedding = DeepFace.represent(face_image, model_name=\"ArcFace\", enforce_detection=False)\n",
    "\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "faces1 = get_embeddings(image_path)\n",
    "print(f\"Number of faces detected: {len(faces1)}\")\n",
    "print(f\"Shape of each embedding: {np.array(faces1[0]).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = Image.open(image_path)\n",
    "image2 = Image.open(\"./datasets/people/61705984-3729-47CB-A20B-DD1CFF9B9BA7_1_105_c.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces1 = get_embeddings(\"./datasets/people/028C2197-6F21-4DBB-A868-AC93EE45A1F9_4_5005_c.jpeg\")\n",
    "faces2 = get_embeddings(\"./datasets/people/61705984-3729-47CB-A20B-DD1CFF9B9BA7_1_105_c.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_resize(img, box, image_size):\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2] - x\n",
    "    h = box[3] - y\n",
    "\n",
    "    out = img.crop(box).copy().resize((image_size, image_size), Image.BILINEAR)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_size(img):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        return img.shape[1::-1]\n",
    "    else:\n",
    "        return img.size\n",
    "\n",
    "\n",
    "def extract_face(img, box, image_size=160, margin=0):\n",
    "    margin = [\n",
    "        margin * (box[2] - box[0]) / (image_size - margin),\n",
    "        margin * (box[3] - box[1]) / (image_size - margin),\n",
    "    ]\n",
    "    raw_image_size = get_size(img)\n",
    "    box = [\n",
    "        int(max(box[0] - margin[0] / 2, 0)),\n",
    "        int(max(box[1] - margin[1] / 2, 0)),\n",
    "        int(min(box[2] + margin[0] / 2, raw_image_size[0])),\n",
    "        int(min(box[3] + margin[1] / 2, raw_image_size[1])),\n",
    "    ]\n",
    "\n",
    "    face = crop_resize(img, box, image_size)\n",
    "\n",
    "    face = np.array(face, dtype=np.float32)\n",
    "\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = np.array(faces1[0][0]['embedding'])\n",
    "emb2 = np.array(faces2[0][0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6702]], dtype=torch.float64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.cos_sim(emb1, emb2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
