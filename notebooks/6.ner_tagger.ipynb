{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags, ne_chunk\n",
    "import spacy\n",
    "import svgling\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, get_scheduler, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /Users/vsaravind/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"maxent_ne_chunker_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner_categories = [\"PERSON\", \"LOC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Show all the photos of Bob and Aravind\"\n",
    "doc = nlp(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Show', 'NNP'), ('all', 'PDT'), ('the', 'DT'), ('photos', 'NN'), ('of', 'IN'), ('Bob', 'NNP'), ('and', 'CC'), ('Aravind', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "sent = pos_tag(word_tokenize(text), lang='eng')\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Chunking (S\n",
      "  (mychunk Show/NNP)\n",
      "  all/PDT\n",
      "  the/DT\n",
      "  (mychunk photos/NN)\n",
      "  of/IN\n",
      "  (mychunk Bob/NNP)\n",
      "  and/CC\n",
      "  (mychunk Aravind/NNP))\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,440.0,168.0\" width=\"440px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"16.3636%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Show</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.18182%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.09091%\" x=\"16.3636%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">all</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PDT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.9091%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.09091%\" x=\"25.4545%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16.3636%\" x=\"34.5455%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">photos</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.7273%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.27273%\" x=\"50.9091%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.5455%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16.3636%\" x=\"58.1818%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Bob</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.3636%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.09091%\" x=\"74.5455%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.0909%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16.3636%\" x=\"83.6364%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Aravind</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.8182%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "TreeLayout(Tree('S', [Tree('mychunk', [('Show', 'NNP')]), ('all', 'PDT'), ('the', 'DT'), Tree('mychunk', [('photos', 'NN')]), ('of', 'IN'), Tree('mychunk', [('Bob', 'NNP')]), ('and', 'CC'), Tree('mychunk', [('Aravind', 'NNP')])]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = \"\"\"mychunk:{<NN.*>+}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "output = chunker.parse(sent)\n",
    "print(\"After Chunking\", output)\n",
    "svgling.draw_tree(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Show', 'NNP', 'B-mychunk'), ('all', 'PDT', 'O'), ('the', 'DT', 'O'), ('photos', 'NN', 'B-mychunk'), ('of', 'IN', 'O'), ('Bob', 'NNP', 'B-mychunk'), ('and', 'CC', 'O'), ('Aravind', 'NNP', 'B-mychunk')]\n"
     ]
    }
   ],
   "source": [
    "iob_tagged = tree2conlltags(output)\n",
    "print(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('PERSON', [('Bob', 'NNP')]), Tree('PERSON', [('Aravind', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "def extract_ne(trees, labels):\n",
    "\n",
    "    ne_list = []\n",
    "    for tree in ne_res:\n",
    "        if hasattr(tree, \"label\"):\n",
    "            if tree.label() in labels:\n",
    "                ne_list.append(tree)\n",
    "    return ne_list\n",
    "\n",
    "\n",
    "ne_res = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "labels = [\"PERSON\"]\n",
    "\n",
    "print(extract_ne(ne_res, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d071ec1070742928cc506a99a21acdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8d8c4499b54897aea7269c5f94dfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42169445f51f4f40919ab70e3a8dc728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57418665824948c8b4cb191b7a7869df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities from the pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsaravind/dev/ChatterChum/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint_default = \"dslim/bert-base-NER\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint_default)\n",
    "model_checkpoint = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "print(\"Entities from the pretrained model\")\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "test_sentence = \"\"\"Show all the photos of bob and jack\"\"\"\n",
    "# Vice President Kamala Harris Tweet 5th July 2022\n",
    "\n",
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "input_ids = torch.tensor([tokenized_sentence])\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "label_indices = np.argmax(output[0].to(\"cpu\").numpy(), axis=2)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to(\"cpu\").numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        new_labels.append(label_idx)\n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {\n",
    "    \"B-PER\": \"B-PERSON\",\n",
    "    \"B-LOC\": \"B-LOCATION\",\n",
    "    \"O\": \"Other\",\n",
    "    \"I-LOC\": \"I-LOCATION\",\n",
    "}\n",
    "tags = []\n",
    "_words = []\n",
    "for token, label in zip(new_tokens, new_labels):\n",
    "    tags.append(dict1[model.config.id2label[label]])\n",
    "    _words.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/vsaravind/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'words' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uncommon_words\n\u001b[1;32m     23\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow all the photos of Bob and Jack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m uncommon_words \u001b[38;5;241m=\u001b[39m \u001b[43mextract_uncommon_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(uncommon_words)\n",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m, in \u001b[0;36mextract_uncommon_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extracts uncommon English words from a given text.\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m english_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mwords\u001b[49m\u001b[38;5;241m.\u001b[39mwords())\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Tokenize the text and convert to lowercase\u001b[39;00m\n\u001b[1;32m      8\u001b[0m words \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'words' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def extract_uncommon_words(text):\n",
    "    \"\"\"Extracts uncommon English words from a given text.\"\"\"\n",
    "\n",
    "    nltk.download(\"words\")\n",
    "    english_words = set(words.words())\n",
    "\n",
    "    # Tokenize the text and convert to lowercase\n",
    "    _words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Filter out non-alphabetic words\n",
    "    _words = [word for word in _words if word.isalpha()]\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_counts = Counter(_words)\n",
    "\n",
    "    # Extract uncommon words\n",
    "    uncommon_words = [\n",
    "        word for word, count in word_counts.items() if word not in english_words or count < 2\n",
    "    ]\n",
    "\n",
    "    return uncommon_words\n",
    "\n",
    "text = \"Show all the photos of Bob and Jack\"\n",
    "uncommon_words = extract_uncommon_words(text)\n",
    "print(uncommon_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "lst = []\n",
    "total_words = []\n",
    "temp = 0\n",
    "for key, i in enumerate(words[1:-1]):\n",
    "    # print(i)\n",
    "    if i not in total_words:\n",
    "        re_string = r\"\\b({})\\b\".format(i)\n",
    "        start = re.search(re_string, test_sentence).start()\n",
    "        end = re.search(re_string, test_sentence).end()\n",
    "        dict1 = {}\n",
    "        dict1[\"start\"] = start + temp\n",
    "        dict1[\"end\"] = end\n",
    "        dict1[\"label\"] = tags[1:-1][key]\n",
    "        dict1[\"Word\"] = i\n",
    "        total_words.append(i)\n",
    "\n",
    "        lst.append(dict1)\n",
    "\n",
    "    else:\n",
    "        start = start + len(words[1:-1][key - 1])\n",
    "        end = end + len(i) + 1\n",
    "\n",
    "        dict1 = {}\n",
    "        dict1[\"start\"] = start\n",
    "        dict1[\"end\"] = end\n",
    "        dict1[\"label\"] = tags[1:-1][key] + \"2\"\n",
    "        dict1[\"Word\"] = i\n",
    "        total_words.append(i)\n",
    "\n",
    "        lst.append(dict1)\n",
    "ex = [{\"text\": test_sentence, \"ents\": lst, \"title\": None}]\n",
    "cols = {\n",
    "    \"B-PERSON\": \"#dad1f6\",\n",
    "    \"I-LOCATION\": \"#adcfad\",\n",
    "    \"OTHER2\": \"#fbbf9a\",\n",
    "    \"B-LOCATION\": \"#bdf2fa\",\n",
    "    \"I-GENE_OR_GENOME\": \"#eea69e\",\n",
    "    \"OTHER\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "}\n",
    "options = {\n",
    "    \"colors\": cols,\n",
    "}\n",
    "\n",
    "html = displacy.render(ex, style=\"ent\", options=options, manual=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
